{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import unsplit.attacks as unsplit\n",
    "from unsplit.models import *\n",
    "from unsplit.util import *\n",
    "\n",
    "import optimizers as opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    trainset = datasets.MNIST('data/mnist', download=True, train=True, transform=transforms.ToTensor())\n",
    "    testset = datasets.MNIST('data/mnist', download=True, train=False, transform=transforms.ToTensor())\n",
    "    client, server, clone = MnistNet(), MnistNet(), MnistNet()\n",
    "elif dataset == 'f_mnist':\n",
    "    trainset = datasets.FashionMNIST('data/f_mnist', download=True, train=True, transform=transforms.ToTensor())\n",
    "    testset = datasets.FashionMNIST('data/f_mnist', download=True, train=False, transform=transforms.ToTensor())\n",
    "    client, server, clone = MnistNet(), MnistNet(), MnistNet()\n",
    "elif dataset == 'cifar':\n",
    "    trainset = datasets.CIFAR10('data/cifar', download=True, train=True, transform=transforms.ToTensor())\n",
    "    testset = datasets.CIFAR10('data/cifar', download=True, train=False, transform=transforms.ToTensor())\n",
    "    client, server, clone = CifarNet(), CifarNet(), CifarNet()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=64)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader:\n",
    "    for image in images:\n",
    "        image = image.to(device)\n",
    "    for label in labels:\n",
    "        label = label.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.302513599395752 Acc: 9.23076923076923\n",
      "Epoch: 1 Loss: 2.2995636463165283 Acc: 9.994538503549972\n",
      "Epoch: 2 Loss: 2.2964916229248047 Acc: 10.399562123700054\n",
      "Epoch: 3 Loss: 2.292752981185913 Acc: 16.5848336061102\n",
      "Epoch: 4 Loss: 2.2878165245056152 Acc: 19.102353585112205\n",
      "Epoch: 5 Loss: 2.2807109355926514 Acc: 30.45379989065063\n",
      "Epoch: 6 Loss: 2.2687489986419678 Acc: 36.862314865606145\n",
      "Epoch: 7 Loss: 2.243864059448242 Acc: 53.87149917627677\n",
      "Epoch: 8 Loss: 2.1818437576293945 Acc: 57.158590308370044\n",
      "Epoch: 9 Loss: 1.9545646905899048 Acc: 68.17685589519651\n",
      "Epoch: 10 Loss: 1.2179653644561768 Acc: 76.1878453038674\n",
      "Epoch: 11 Loss: 0.7220318913459778 Acc: 81.5401419989077\n",
      "Epoch: 12 Loss: 0.5681466460227966 Acc: 85.42576419213974\n",
      "Epoch: 13 Loss: 0.4850541353225708 Acc: 86.66666666666667\n",
      "Epoch: 14 Loss: 0.4297293722629547 Acc: 88.14773980154355\n",
      "Epoch: 15 Loss: 0.38810059428215027 Acc: 89.62162162162163\n",
      "Epoch: 16 Loss: 0.3556075692176819 Acc: 90.46575342465754\n",
      "Epoch: 17 Loss: 0.3295515477657318 Acc: 90.85432639649507\n",
      "Epoch: 18 Loss: 0.3077648878097534 Acc: 92.31619679380873\n",
      "Epoch: 19 Loss: 0.2890545427799225 Acc: 91.90974133186572\n",
      "Epoch: 20 Loss: 0.27336829900741577 Acc: 91.50110375275938\n",
      "Epoch: 21 Loss: 0.25876733660697937 Acc: 91.97598253275109\n",
      "Epoch: 22 Loss: 0.2459271401166916 Acc: 93.1656642974303\n",
      "Epoch: 23 Loss: 0.23418712615966797 Acc: 93.4640522875817\n",
      "Epoch: 24 Loss: 0.22435900568962097 Acc: 93.52360043907794\n",
      "Epoch: 25 Loss: 0.21501071751117706 Acc: 94.30309734513274\n",
      "Epoch: 26 Loss: 0.2068350911140442 Acc: 93.02452316076294\n",
      "Epoch: 27 Loss: 0.1989363431930542 Acc: 94.41062534587715\n",
      "Epoch: 28 Loss: 0.19207048416137695 Acc: 94.02324294410626\n",
      "Epoch: 29 Loss: 0.18540780246257782 Acc: 95.00542888165037\n",
      "Epoch: 30 Loss: 0.17940998077392578 Acc: 94.52954048140043\n",
      "Epoch: 31 Loss: 0.17337891459465027 Acc: 95.76601671309193\n",
      "Epoch: 32 Loss: 0.1685652881860733 Acc: 95.36167863059083\n",
      "Epoch: 33 Loss: 0.16351103782653809 Acc: 95.19125683060109\n",
      "Epoch: 34 Loss: 0.15895116329193115 Acc: 95.17615176151762\n",
      "Epoch: 35 Loss: 0.154740571975708 Acc: 96.94822888283379\n",
      "Epoch: 36 Loss: 0.1506946086883545 Acc: 96.26623376623377\n",
      "Epoch: 37 Loss: 0.14649233222007751 Acc: 95.76224545954871\n",
      "Epoch: 38 Loss: 0.14264845848083496 Acc: 96.00219058050384\n",
      "Epoch: 39 Loss: 0.13960759341716766 Acc: 96.1977186311787\n",
      "Epoch: 40 Loss: 0.13623777031898499 Acc: 97.06999457406403\n",
      "Epoch: 41 Loss: 0.1325995773077011 Acc: 96.51355838406198\n",
      "Epoch: 42 Loss: 0.1299997866153717 Acc: 96.19834710743801\n",
      "Epoch: 43 Loss: 0.12708711624145508 Acc: 95.96069868995633\n",
      "Epoch: 44 Loss: 0.1244584172964096 Acc: 96.22950819672131\n",
      "Epoch: 45 Loss: 0.12198090553283691 Acc: 96.32272228320527\n",
      "Epoch: 46 Loss: 0.11976291984319687 Acc: 96.39344262295081\n",
      "Epoch: 47 Loss: 0.11740744113922119 Acc: 96.64281783159053\n",
      "Epoch: 48 Loss: 0.11478637903928757 Acc: 97.23604201216142\n",
      "Epoch: 49 Loss: 0.11290791630744934 Acc: 96.92814042786615\n"
     ]
    }
   ],
   "source": [
    "client_opt = torch.optim.SGD(client.parameters(), lr=0.001)\n",
    "server_opt = torch.optim.SGD(server.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        client_opt.zero_grad()\n",
    "        server_opt.zero_grad()\n",
    "\n",
    "        pred = server(client(images, end=split_layer), start=split_layer+1)\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        running_loss += loss\n",
    "\n",
    "        server_opt.step()\n",
    "        client_opt.step()\n",
    "    else:\n",
    "        print(f'Epoch: {epoch} Loss: {running_loss / len(trainloader)} Acc: {get_test_score(client, server, testset, split=split_layer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "class CustomNormalDistribution(nn.Module):\n",
    "    def __init__(self, shape, mean, std):\n",
    "        super(CustomNormalDistribution, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.mean = nn.Parameter(torch.tensor(mean))\n",
    "        self.std = nn.Parameter(torch.tensor(std))\n",
    "\n",
    "    def forward(self):\n",
    "        return dist.Normal(self.mean, self.std)\n",
    "\n",
    "\n",
    "def noisybatch(images, mean=0.0, std=1.0):\n",
    "    shape = images.shape\n",
    "    custom_dist = CustomNormalDistribution(shape, mean, std)\n",
    "    sample = custom_dist().rsample()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find global $l2$-sensitivity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us adjust distribution parameter $\\sigma$ for $(\\varepsilon, \\delta)$-DP.\n",
    "\n",
    "Code is taken from https://github.com/BorjaBalle/analytic-gaussian-mechanism/blob/master/agm-example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, sqrt\n",
    "from scipy.special import erf\n",
    "\n",
    "def calibrateAnalyticGaussianMechanism(epsilon, delta, GS, tol = 1.e-12):\n",
    "    \"\"\" Calibrate a Gaussian perturbation for differential privacy using the analytic Gaussian mechanism of [Balle and Wang, ICML'18]\n",
    "\n",
    "    Arguments:\n",
    "    epsilon : target epsilon (epsilon > 0)\n",
    "    delta : target delta (0 < delta < 1)\n",
    "    GS : upper bound on L2 global sensitivity (GS >= 0)\n",
    "    tol : error tolerance for binary search (tol > 0)\n",
    "\n",
    "    Output:\n",
    "    sigma : standard deviation of Gaussian noise needed to achieve (epsilon,delta)-DP under global sensitivity GS\n",
    "    \"\"\"\n",
    "\n",
    "    def Phi(t):\n",
    "        return 0.5*(1.0 + erf(float(t)/sqrt(2.0)))\n",
    "\n",
    "    def caseA(epsilon,s):\n",
    "        return Phi(sqrt(epsilon*s)) - exp(epsilon)*Phi(-sqrt(epsilon*(s+2.0)))\n",
    "\n",
    "    def caseB(epsilon,s):\n",
    "        return Phi(-sqrt(epsilon*s)) - exp(epsilon)*Phi(-sqrt(epsilon*(s+2.0)))\n",
    "\n",
    "    def doubling_trick(predicate_stop, s_inf, s_sup):\n",
    "        while(not predicate_stop(s_sup)):\n",
    "            s_inf = s_sup\n",
    "            s_sup = 2.0*s_inf\n",
    "        return s_inf, s_sup\n",
    "\n",
    "    def binary_search(predicate_stop, predicate_left, s_inf, s_sup):\n",
    "        s_mid = s_inf + (s_sup-s_inf)/2.0\n",
    "        while(not predicate_stop(s_mid)):\n",
    "            if (predicate_left(s_mid)):\n",
    "                s_sup = s_mid\n",
    "            else:\n",
    "                s_inf = s_mid\n",
    "            s_mid = s_inf + (s_sup-s_inf)/2.0\n",
    "        return s_mid\n",
    "\n",
    "    delta_thr = caseA(epsilon, 0.0)\n",
    "\n",
    "    if (delta == delta_thr):\n",
    "        alpha = 1.0\n",
    "\n",
    "    else:\n",
    "        if (delta > delta_thr):\n",
    "            predicate_stop_DT = lambda s : caseA(epsilon, s) >= delta\n",
    "            function_s_to_delta = lambda s : caseA(epsilon, s)\n",
    "            predicate_left_BS = lambda s : function_s_to_delta(s) > delta\n",
    "            function_s_to_alpha = lambda s : sqrt(1.0 + s/2.0) - sqrt(s/2.0)\n",
    "\n",
    "        else:\n",
    "            predicate_stop_DT = lambda s : caseB(epsilon, s) <= delta\n",
    "            function_s_to_delta = lambda s : caseB(epsilon, s)\n",
    "            predicate_left_BS = lambda s : function_s_to_delta(s) < delta\n",
    "            function_s_to_alpha = lambda s : sqrt(1.0 + s/2.0) + sqrt(s/2.0)\n",
    "\n",
    "        predicate_stop_BS = lambda s : abs(function_s_to_delta(s) - delta) <= tol\n",
    "\n",
    "        s_inf, s_sup = doubling_trick(predicate_stop_DT, 0.0, 1.0)\n",
    "        s_final = binary_search(predicate_stop_BS, predicate_left_BS, s_inf, s_sup)\n",
    "        alpha = function_s_to_alpha(s_final)\n",
    "        \n",
    "    sigma = alpha*GS/sqrt(2.0*epsilon)\n",
    "\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5909175992591167"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = calibrateAnalyticGaussianMechanism(0.5, 0.5, 1.0)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.3023247718811035 Acc: 10.757409440175632\n",
      "Epoch: 1 Loss: 2.3000195026397705 Acc: 9.764125068568294\n",
      "Epoch: 2 Loss: 2.298835515975952 Acc: 11.95414847161572\n",
      "Epoch: 3 Loss: 2.2971088886260986 Acc: 14.144736842105264\n",
      "Epoch: 4 Loss: 2.295240879058838 Acc: 18.61353711790393\n",
      "Epoch: 5 Loss: 2.2938835620880127 Acc: 18.489727928928374\n",
      "Epoch: 6 Loss: 2.2917065620422363 Acc: 16.602528862012093\n",
      "Epoch: 7 Loss: 2.2879931926727295 Acc: 19.493392070484582\n",
      "Epoch: 8 Loss: 2.28412127494812 Acc: 26.464088397790054\n",
      "Epoch: 9 Loss: 2.2770228385925293 Acc: 33.91589295466958\n",
      "Epoch: 10 Loss: 2.2696893215179443 Acc: 38.58397365532382\n",
      "Epoch: 11 Loss: 2.254446268081665 Acc: 39.33701657458563\n",
      "Epoch: 12 Loss: 2.2243704795837402 Acc: 45.7347275729224\n",
      "Epoch: 13 Loss: 2.1799306869506836 Acc: 52.05704882062534\n",
      "Epoch: 14 Loss: 2.0922393798828125 Acc: 55.14425694066413\n",
      "Epoch: 15 Loss: 1.9192321300506592 Acc: 61.06243154435926\n",
      "Epoch: 16 Loss: 1.6574550867080688 Acc: 65.00274273176083\n",
      "Epoch: 17 Loss: 1.3882473707199097 Acc: 76.27397260273973\n",
      "Epoch: 18 Loss: 1.1220201253890991 Acc: 81.58609451385117\n",
      "Epoch: 19 Loss: 0.8551625609397888 Acc: 86.01973684210526\n",
      "Epoch: 20 Loss: 0.6666088700294495 Acc: 86.51315789473684\n",
      "Epoch: 21 Loss: 0.5705045461654663 Acc: 87.65840220385675\n",
      "Epoch: 22 Loss: 0.49746406078338623 Acc: 88.53434288804759\n",
      "Epoch: 23 Loss: 0.4459424912929535 Acc: 90.38776624795194\n",
      "Epoch: 24 Loss: 0.4441465735435486 Acc: 89.082020640956\n",
      "Epoch: 25 Loss: 0.4017939269542694 Acc: 90.07675438596492\n",
      "Epoch: 26 Loss: 0.36785078048706055 Acc: 91.08910891089108\n",
      "Epoch: 27 Loss: 0.3447017967700958 Acc: 91.53468050245768\n",
      "Epoch: 28 Loss: 0.3320876657962799 Acc: 91.51515151515152\n",
      "Epoch: 29 Loss: 0.3214375376701355 Acc: 92.69356597600873\n",
      "Epoch: 30 Loss: 0.3053957521915436 Acc: 92.73927392739274\n",
      "Epoch: 31 Loss: 0.2969001233577728 Acc: 93.07146753955264\n",
      "Epoch: 32 Loss: 0.2806713581085205 Acc: 92.86109573879358\n",
      "Epoch: 33 Loss: 0.3438331186771393 Acc: 91.8992884510126\n",
      "Epoch: 34 Loss: 0.304751455783844 Acc: 93.0540242557883\n",
      "Epoch: 35 Loss: 0.2739235460758209 Acc: 94.282572842221\n",
      "Epoch: 36 Loss: 0.2626912593841553 Acc: 94.03567447045708\n",
      "Epoch: 37 Loss: 0.25785762071609497 Acc: 93.35548172757476\n",
      "Epoch: 38 Loss: 0.2450520545244217 Acc: 93.53778751369113\n",
      "Epoch: 39 Loss: 0.23460832238197327 Acc: 94.16941694169417\n",
      "Epoch: 40 Loss: 0.22904765605926514 Acc: 94.49035812672176\n",
      "Epoch: 41 Loss: 0.22088350355625153 Acc: 93.47229840921558\n",
      "Epoch: 42 Loss: 0.2116955816745758 Acc: 95.82417582417582\n",
      "Epoch: 43 Loss: 0.2065035104751587 Acc: 95.509977827051\n",
      "Epoch: 44 Loss: 0.21094341576099396 Acc: 94.3801652892562\n",
      "Epoch: 45 Loss: 0.19437889754772186 Acc: 94.91993373826615\n",
      "Epoch: 46 Loss: 0.1912895143032074 Acc: 94.32933478735005\n",
      "Epoch: 47 Loss: 0.1904347538948059 Acc: 93.6951754385965\n",
      "Epoch: 48 Loss: 0.18223613500595093 Acc: 94.76584022038567\n",
      "Epoch: 49 Loss: 0.18188658356666565 Acc: 93.91592920353982\n"
     ]
    }
   ],
   "source": [
    "client_opt = torch.optim.SGD(client.parameters(), lr=0.001)\n",
    "server_opt = torch.optim.SGD(server.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        client_opt.zero_grad()\n",
    "        server_opt.zero_grad()\n",
    "        noise = noisybatch(images, mean=0.0, std=sigma)\n",
    "        pred = server(client(torch.add(images, noise), end=split_layer), start=split_layer+1)\n",
    "\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        running_loss += loss\n",
    "\n",
    "        server_opt.step()\n",
    "        client_opt.step()\n",
    "    else:\n",
    "        print(f'Epoch: {epoch} Loss: {running_loss / len(trainloader)} Acc: {get_test_score(client, server, testset, split=split_layer)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
